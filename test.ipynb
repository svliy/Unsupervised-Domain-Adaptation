{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(config):\n",
    "    AUs_num = config.DATA.CLASS_NUM\n",
    "    aus = ['AU'+str(au_name) for au_name in config.DATA.AU_LIST]\n",
    "    with open(config.DATA.SOURCE.TRAIN_LIST, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        labels = np.array([[int(row[au]) for au in aus] for row in reader])\n",
    "        all_ = [len(labels)] * AUs_num\n",
    "        positive = np.sum(labels, 0)\n",
    "\n",
    "    negative = np.array(all_) - positive\n",
    "\n",
    "    weight_cls = WeightNorm(positive.tolist(), negative.tolist())\n",
    "    norm_weight = weight_cls.normalize()\n",
    "    norm_weight =  np.array(norm_weight)\n",
    "    norm_weight = norm_weight.tolist()\n",
    "    norm_weight = torch.FloatTensor(norm_weight)\n",
    "    return norm_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 修改tensor的维度的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.reshape\n",
    "\n",
    "x1 = torch.randn((128, 512, 7, 7))\n",
    "x2 = torch.randn((128, 512, 7, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([x1, x2]).shape  # (2, 128, 512, 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 余弦退火算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(256, 5)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [scheduler.get_last_lr()[0]]\n",
    "print(f\"Initial Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "for epoch in range(epochs):\n",
    "    # 训练模型\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 更新学习率\n",
    "    scheduler.step()\n",
    "    \n",
    "    # 打印当前学习率\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    y.append(scheduler.get_last_lr()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs+1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming and augmenting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = 32, 32\n",
    "img = torch.randint(0, 256, size=(3, H, W), dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 3, requires_grad=True)\n",
    "target = torch.randint(3, (3,), dtype=torch.int64)\n",
    "loss = F.cross_entropy(input, target, reduction='none')\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randint(2, (3, 3), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1 = input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1_log_softmax = F.log_softmax(input)\n",
    "row_1_log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.nll_loss(row_1_log_softmax, target, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target /target.sum(dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1_log_softmax[target.bool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = -torch.where(target.bool(), row_1_log_softmax, torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.sum(dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.sum(dim=1)  / target.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.div(12, 0+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_matrix_rows_no_loop(pseudo_label, labels):\n",
    "    row_equal = torch.eq(pseudo_label, labels).all(dim=1) # 比较每一行\n",
    "    count = torch.sum(row_equal).item() #计算相等行数\n",
    "    print(f\"Count: {count}\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pseudo_label = torch.tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "                             [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                             [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
    "labels = torch.tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "                       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
    "\n",
    "compare_matrix_rows_no_loop(pseudo_label, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'available_models',\n",
       " 'clip',\n",
       " 'load',\n",
       " 'model',\n",
       " 'simple_tokenizer',\n",
       " 'tokenize']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RN50中没有dropout\n",
    "# 参数冻结的CLIP模型中\n",
    "clip_model, _ = clip.load('RN101', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = torch.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, x3, x4, out = clip_model.visual(data_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of x1 is: torch.Size([1, 256, 56, 56])\n",
      "The shape of x2 is: torch.Size([1, 512, 28, 28])\n",
      "The shape of x3 is: torch.Size([1, 1024, 14, 14])\n",
      "The shape of x4 is: torch.Size([1, 2048, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of x1 is: {x1.shape}\")\n",
    "print(f\"The shape of x2 is: {x2.shape}\")\n",
    "print(f\"The shape of x3 is: {x3.shape}\")\n",
    "print(f\"The shape of x4 is: {x4.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_model\n",
    "# clip\n",
    "# ----visual\n",
    "# ----transformer\n",
    "# ----token_embedding\n",
    "# ----ln_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in clip_model.modules():\n",
    "    if isinstance(module, nn.Dropout):\n",
    "        print(f\"{module}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_model.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn(1, 3, 224, 224)\n",
    "output = clip_model.encode_image(images)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习率调度策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "\n",
    "epochs = 30\n",
    "model = nn.Linear(200, 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"E({epoch}): lr:{scheduler.get_last_lr()}\")\n",
    "    lr_list.append(scheduler.get_last_lr()[0])\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设你已经定义好了模型、数据集和数据加载器\n",
    "# 这里的 dataloader 是一个模拟，实际使用时请替换\n",
    "class DummyDataset(torch.utils.data.Dataset):\n",
    "    def __len__(self):\n",
    "        return 100 # 假设每个 epoch 有 100 个 batch/iteration\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.randn(10), torch.randint(0, 2, (1,)) # 假的输入和标签\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(DummyDataset(), batch_size=10) # 假设 batch_size 为 10\n",
    "\n",
    "# 1. 定义超参数\n",
    "total_epochs = 50 # 总的训练 epoch 数，请根据你的任务调整\n",
    "initial_lr = 0.0001 # 直接从这个学习率开始衰减\n",
    "eta_min_cosine = 0 # Cosine Annealing 衰减到的最小学习率 (可以设为0或一个很小的值)\n",
    "\n",
    "# 假设模型的参数数量为 10\n",
    "dummy_params = [torch.randn(10, 1, requires_grad=True)]\n",
    "\n",
    "# 2. 初始化优化器\n",
    "# 优化器直接使用初始学习率\n",
    "optimizer = optim.Adam(dummy_params, lr=initial_lr)\n",
    "\n",
    "# 3. 计算总的迭代步数\n",
    "steps_per_epoch = len(train_dataloader)\n",
    "total_iterations = total_epochs * steps_per_epoch\n",
    "\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Total iterations: {total_iterations}\")\n",
    "\n",
    "# 4. 初始化 CosineAnnealingLR 调度器\n",
    "# T_max 是 Cosine Annealing 周期所需的总步数\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_iterations, eta_min=eta_min_cosine)\n",
    "\n",
    "# 5. 训练循环\n",
    "global_step = 0 # 全局迭代步数计数器\n",
    "lr_history_cosine = [] # 用于记录学习率变化\n",
    "\n",
    "print(\"\\nSimulating training steps with Cosine Annealing Decay:\")\n",
    "for epoch in range(total_epochs):\n",
    "    # model.train() # 设置模型为训练模式\n",
    "    # for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "    for i in range(steps_per_epoch): # 使用模拟的循环\n",
    "        # 前向传播、计算损失、反向传播 (模拟这些步骤)\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "\n",
    "        optimizer.step() # 模拟参数更新\n",
    "        optimizer.zero_grad() # 模拟梯度清零\n",
    "\n",
    "        # --- 调度器更新 ---\n",
    "        # 在 optimizer.step() 之后调用 scheduler.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # 记录当前学习率\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        lr_history_cosine.append(current_lr)\n",
    "\n",
    "        # 递增全局步数计数器\n",
    "        global_step += 1\n",
    "\n",
    "        # 打印当前学习率 (可选，用于调试)\n",
    "        # if (global_step) % 10 == 0 or global_step <= 5 or global_step > total_iterations - 5:\n",
    "        #      print(f\"Step {global_step}: LR = {current_lr:.8f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch} finished. Current LR: {optimizer.param_groups[0]['lr']:.8f}\")\n",
    "\n",
    "print(\"\\nCosine Annealing Decay Simulation finished.\")\n",
    "\n",
    "# 6. 可视化学习率变化\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, total_iterations + 1), lr_history_cosine)\n",
    "plt.title('Learning Rate Schedule: Cosine Annealing Decay')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 领域自适应中遍历数据集的写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetTest(Dataset):\n",
    "    \n",
    "    def __init__(self, data, label):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        result = self.data[index]\n",
    "        return result, self.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = DataSetTest(np.arange(50), 0)\n",
    "target_dataset = DataSetTest(np.arange(120), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_loader = DataLoader(source_dataset, batch_size=25)\n",
    "target_loader = DataLoader(target_dataset, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in source_loader:\n",
    "    print(f\"{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in target_loader:\n",
    "    print(f\"{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这种办法不行的\n",
    "for x,y in zip(source_loader, target_loader):\n",
    "    print(f\"{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在一个迭代器耗尽时，重新创建该迭代器。\n",
    "source_iter = iter(source_loader)\n",
    "for batch_idx, (x_t, y_t) in enumerate(target_loader): # 放target以尽可能的利用目标域的无标签数据\n",
    "    try:\n",
    "        x_s, y_s = next(source_iter)\n",
    "    except:\n",
    "        source_iter = iter(source_loader)\n",
    "        x_s, y_s = next(source_iter)\n",
    "    print(f\"batch: {batch_idx}\")\n",
    "    print(f\"t_x: {x_t}\")\n",
    "    print(f\"s_x: {x_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_matrix = np.load('/mnt/sda/yiren/code/uda/Unsupervised-Domain-Adaptation/results/gft2bp4d+/two_train_cross_fold_3_seed_1000_date_2025-05-10_12_16-59_PM/acc_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for acc in acc_matrix:\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6)) # 设置图像大小\n",
    "sns.heatmap(\n",
    "    acc_matrix*100,\n",
    "    annot=True,        # 在单元格上显示数值\n",
    "    fmt=\".2f\",         # 格式化字符串，.2f 表示保留两位小数的浮点数\n",
    "    cmap=\"viridis\",    # 选择一个颜色映射 (e.g., \"viridis\", \"coolwarm\", \"YlGnBu\")\n",
    "    linewidths=.5,     # 单元格之间的线条宽度\n",
    "    cbar=True,         # 显示颜色条\n",
    "    square=True        # 使单元格为正方形 (如果行数和列数相近)\n",
    ")\n",
    "plt.title(\"Heatmap of acc_matrix (保留两位小数)\")\n",
    "plt.xlabel(\"列索引 (Column Index)\") # 或者你的列标签\n",
    "plt.ylabel(\"行索引 (Row Index)\")   # 或者你的行标签\n",
    "\n",
    "# 如果你有行和列的标签，可以这样设置：\n",
    "# row_labels = [f\"Row {i+1}\" for i in range(acc_matrix.shape[0])]\n",
    "# col_labels = [f\"Col {j+1}\" for j in range(acc_matrix.shape[1])]\n",
    "# sns.heatmap(..., xticklabels=col_labels, yticklabels=row_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 伪标签筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_std_list = np.load('/mnt/sda/yiren/code/uda/Unsupervised-Domain-Adaptation/results/disfa2bp4d/two_train_cross_fold_2_seed_1000_date_2025-05-09_05_12-06_PM/out_std_list.npy', allow_pickle=True)\n",
    "out_prob_list = np.load('/mnt/sda/yiren/code/uda/Unsupervised-Domain-Adaptation/results/disfa2bp4d/two_train_cross_fold_2_seed_1000_date_2025-05-09_05_12-06_PM/out_prob_list.npy', allow_pickle=True)\n",
    "\n",
    "images_list = np.load('/mnt/sda/yiren/code/uda/Unsupervised-Domain-Adaptation/results/disfa2bp4d/two_train_cross_fold_2_seed_1000_date_2025-05-09_05_12-06_PM/images_list.npy', allow_pickle=True)\n",
    "truth_labels = np.load('/mnt/sda/yiren/code/uda/Unsupervised-Domain-Adaptation/results/disfa2bp4d/two_train_cross_fold_2_seed_1000_date_2025-05-09_05_12-06_PM/truth_labels.npy', allow_pickle=True)\n",
    "pseudo_labels = np.load('/mnt/sda/yiren/code/uda/Unsupervised-Domain-Adaptation/results/disfa2bp4d/two_train_cross_fold_2_seed_1000_date_2025-05-09_05_12-06_PM/pseudo_labels.npy', allow_pickle=True)\n",
    "\n",
    "acc_matrix = np.load('/mnt/sda/yiren/code/uda/Unsupervised-Domain-Adaptation/results/disfa2bp4d/two_train_cross_fold_2_seed_1000_date_2025-05-09_05_12-06_PM/acc_matrix.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_std_list.shape, out_prob_list.shape, images_list.shape, truth_labels.shape, pseudo_labels.shape, acc_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of completelpy correct samples: 36004\n",
    "# Proportion of completely correct samples: 0.38669903121173715\n",
    "# accuracy_per_au: [0.7886388  0.8292054  0.81597316 0.78358    0.8070801 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(out_std_list), np.min(out_std_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_p = 0.7\n",
    "kappa_p = 0.05\n",
    "\n",
    "\n",
    "pseudo_labels = (out_std_list<=kappa_p)*(out_prob_list>=tau_p)\n",
    "n_correct_pos = (truth_labels*pseudo_labels).sum(0)\n",
    "n_correct_pos / truth_labels.sum(0)\n",
    "\n",
    "# 伪标签准确率\n",
    "completely_correct_samples_mask = np.all(pseudo_labels == truth_labels, axis=1)\n",
    "num_completely_correct_samples = np.sum(completely_correct_samples_mask)\n",
    "pl_acc = num_completely_correct_samples/len(pseudo_labels)\n",
    "print(f\"伪标签准确率: {pl_acc}\")\n",
    "print(f\"真实标签中值为1且预测为1的准确率：{n_correct_pos / truth_labels.sum(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_p = 0.7\n",
    "kappa_p = 0.05\n",
    "tau_n = 0.05\n",
    "kappa_n = 0.005\n",
    "\n",
    "mean_probs = out_prob_list\n",
    "std_probs = out_std_list\n",
    "# [batch_size, num_classes] boolean tensor for selected positive pseudo-labels\n",
    "# 条件1: 置信度足够高\n",
    "cond_confidence_pos = (mean_probs >= tau_p)\n",
    "# 条件2: 不确定性足够低 (如果启用)\n",
    "cond_uncertainty_pos = (std_probs < kappa_p)\n",
    "selected_positive_pseudo_labels_mask = cond_confidence_pos & cond_uncertainty_pos\n",
    "\n",
    "# selected_positive_pseudo_labels_mask 现在是一个布尔张量，True表示该类别被选为正伪标签\n",
    "\n",
    "# --- 2. 负伪标签筛选 (Negative Pseudo-label Selection) ---\n",
    "\n",
    "# [batch_size, num_classes] boolean tensor for selected negative pseudo-labels\n",
    "# 条件1: (负)置信度足够高，即存在概率足够低\n",
    "cond_confidence_neg = (mean_probs < tau_n)\n",
    "# 条件2: 不确定性足够低 (如果启用)\n",
    "cond_uncertainty_neg = (std_probs < kappa_n) # Assuming std_probs for negative is calculated similarly\n",
    "selected_negative_pseudo_labels_mask = cond_confidence_neg & cond_uncertainty_neg\n",
    "\n",
    "# 冲突解决: 如果一个类别被选为正，则不能被选为负\n",
    "selected_negative_pseudo_labels_mask[selected_positive_pseudo_labels_mask] = False\n",
    "\n",
    "\n",
    "# --- 3. 评估伪标签质量 (Converting to NumPy for your snippet's logic if needed) ---\n",
    "# 为了使用您提供的 NumPy 代码段，我们将 PyTorch 张量转换为 NumPy 数组\n",
    "# 注意：在实际训练循环中，尽量保持数据在 GPU 上的 PyTorch 张量中以提高效率\n",
    "\n",
    "# Positive pseudo-labels\n",
    "pseudo_labels_pos_np = selected_positive_pseudo_labels_mask\n",
    "truth_labels_np = truth_labels # 确保 truth_labels_tensor 是你的真实标签\n",
    "\n",
    "# Negative pseudo-labels\n",
    "pseudo_labels_neg_np = selected_negative_pseudo_labels_mask\n",
    "\n",
    "\n",
    "# --- 评估正伪标签 ---\n",
    "# \"真实标签中值为1且预测为1的准确率\" (This is essentially Precision for the positive class predictions)\n",
    "# More accurately, this is recall of positive labels among those selected as pseudo-labels,\n",
    "# or precision of pseudo-labels if we consider pseudo_labels_pos_np as predictions.\n",
    "# Let's re-interpret based on common metrics.\n",
    "\n",
    "# True Positives (TP): GT is 1, Pseudo is 1\n",
    "tp_pos = (truth_labels_np == 1) & (pseudo_labels_pos_np == 1)\n",
    "# Predicted Positives (PP): Pseudo is 1 (i.e., selected as positive pseudo-label)\n",
    "pp_pos = (pseudo_labels_pos_np == 1)\n",
    "# Actual Positives (AP): GT is 1\n",
    "ap_pos = (truth_labels_np == 1)\n",
    "\n",
    "# Precision of positive pseudo-labels: Of all labels chosen as positive pseudo-labels, how many were actually positive?\n",
    "# (Sum over all samples and classes)\n",
    "precision_positive_pseudo = tp_pos.sum() / pp_pos.sum() if pp_pos.sum() > 0 else 0.0\n",
    "print(f\"正伪标签精确率 (Precision for positive pseudo-labels): {precision_positive_pseudo * 100:.2f}%\")\n",
    "\n",
    "# Recall of positive pseudo-labels: Of all actual positive labels, how many were selected as positive pseudo-labels?\n",
    "recall_positive_pseudo = tp_pos.sum() / ap_pos.sum() if ap_pos.sum() > 0 else 0.0\n",
    "print(f\"正伪标签召回率 (Recall for positive pseudo-labels): {recall_positive_pseudo * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# \"伪标签准确率\" (Sample-level exact match for positive pseudo-labels)\n",
    "# This metric considers a sample's positive pseudo-labels entirely correct ONLY IF the selected positive pseudo-label mask\n",
    "# for that sample exactly matches the ground truth mask FOR THE POSITIONS WHERE PSEUDO-LABELS WERE MADE.\n",
    "# Or, if you mean exact match of the full multi-hot vector:\n",
    "# For positive pseudo-labels, this is tricky. `pseudo_labels_pos_np` only marks selected positives.\n",
    "# A more meaningful sample-level accuracy for multi-label is often Jaccard index or Hamming accuracy.\n",
    "# Your definition \"completely_correct_samples_mask = np.all(pseudo_labels == truth_labels, axis=1)\"\n",
    "# implies comparing the *full* pseudo_label mask (which might include 0s where no decision was made or decision was negative)\n",
    "# with the *full* truth_label mask.\n",
    "# Let's define `pseudo_labels` as a combined decision for clarity for this metric.\n",
    "# For simplicity, let's evaluate based on the *selected positive pseudo-labels* only.\n",
    "# If you want an overall pseudo-label (combining positive and implicitly negative where not positive):\n",
    "combined_pseudo_for_acc = pseudo_labels_pos_np # Where 1 is positive, 0 is not selected as positive.\n",
    "completely_correct_samples_mask_pos = np.all(combined_pseudo_for_acc == truth_labels_np, axis=1)\n",
    "num_completely_correct_samples_pos = np.sum(completely_correct_samples_mask_pos)\n",
    "pl_acc_sample_level_pos = num_completely_correct_samples_pos / len(truth_labels_np) if len(truth_labels_np) > 0 else 0.0\n",
    "print(f\"基于正伪标签的样本级完全匹配准确率: {pl_acc_sample_level_pos * 100:.2f}%\")\n",
    "# This sample-level exact match is very strict for multi-label.\n",
    "\n",
    "\n",
    "# --- 评估负伪标签 ---\n",
    "# True Negatives (TN): GT is 0, Pseudo_neg is 1 (selected as negative)\n",
    "tn_neg = (truth_labels_np == 0) & (pseudo_labels_neg_np == 1)\n",
    "# Predicted Negatives (PN_sel): Pseudo_neg is 1 (selected as negative)\n",
    "pn_sel_neg = (pseudo_labels_neg_np == 1)\n",
    "# Actual Negatives (AN): GT is 0\n",
    "an_neg = (truth_labels_np == 0)\n",
    "\n",
    "# Accuracy of negative pseudo-labels: Of all labels chosen as negative pseudo-labels, how many were actually negative?\n",
    "accuracy_negative_pseudo = tn_neg.sum() / pn_sel_neg.sum() if pn_sel_neg.sum() > 0 else 0.0\n",
    "print(f\"负伪标签准确率 (Accuracy for negative pseudo-labels): {accuracy_negative_pseudo * 100:.2f}%\")\n",
    "\n",
    "# Recall for negative pseudo-labels (how many of the true negatives were captured by negative pseudo-labeling)\n",
    "recall_negative_pseudo = tn_neg.sum() / an_neg.sum() if an_neg.sum() > 0 else 0.0\n",
    "print(f\"负伪标签召回率 (Recall for negative pseudo-labels): {recall_negative_pseudo * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# --- 整合到返回字典 (示例) ---\n",
    "# 筛选后的正伪标签及其原始索引\n",
    "# (假设你有一个 `indexs_tensor` 对应当前批次的原始样本索引)\n",
    "# samples_with_any_pos_pseudo = selected_positive_pseudo_labels_mask.any(dim=1)\n",
    "# selected_pos_indices_in_batch = samples_with_any_pos_pseudo.nonzero(as_tuple=True)[0]\n",
    "# original_indices_for_pos = indexs_tensor[selected_pos_indices_in_batch]\n",
    "# final_positive_pseudo_targets = selected_positive_pseudo_labels_mask[selected_pos_indices_in_batch]\n",
    "\n",
    "# 筛选后的负伪标签及其原始索引\n",
    "# samples_with_any_neg_pseudo = selected_negative_pseudo_labels_mask.any(dim=1)\n",
    "# selected_neg_indices_in_batch = samples_with_any_neg_pseudo.nonzero(as_tuple=True)[0]\n",
    "# original_indices_for_neg = indexs_tensor[selected_neg_indices_in_batch]\n",
    "# final_negative_pseudo_masks = selected_negative_pseudo_labels_mask[selected_neg_indices_in_batch]\n",
    "\n",
    "# pseudo_label_dict = {\n",
    "#     'pseudo_idx_positive': original_indices_for_pos.cpu().tolist() if samples_with_any_pos_pseudo.any() else [],\n",
    "#     'pseudo_target_positive': final_positive_pseudo_targets.cpu().tolist() if samples_with_any_pos_pseudo.any() else [],\n",
    "#     'pseudo_idx_negative': original_indices_for_neg.cpu().tolist() if samples_with_any_neg_pseudo.any() else [],\n",
    "#     'pseudo_mask_negative': final_negative_pseudo_masks.cpu().tolist() if samples_with_any_neg_pseudo.any() else []\n",
    "# }\n",
    "\n",
    "# print(\"Generated pseudo_label_dict:\", pseudo_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_labels_pos_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 真实标签的分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_labels_au1 = truth_labels[:, 0]\n",
    "truth_labels_au2 = truth_labels[:, 1]\n",
    "truth_labels_au4 = truth_labels[:, 2]\n",
    "truth_labels_au6 = truth_labels[:, 3]\n",
    "truth_labels_au12 = truth_labels[:, 4]\n",
    "\n",
    "truth_labels_au1_num_0 = np.sum(truth_labels_au1 == 0)\n",
    "truth_labels_au1_num_1 = np.sum(truth_labels_au1 == 1)\n",
    "\n",
    "truth_labels_au2_num_0 = np.sum(truth_labels_au2 == 0)\n",
    "truth_labels_au2_num_1 = np.sum(truth_labels_au2 == 1)\n",
    "\n",
    "truth_labels_au4_num_0 = np.sum(truth_labels_au4 == 0)\n",
    "truth_labels_au4_num_1 = np.sum(truth_labels_au4 == 1)\n",
    "\n",
    "truth_labels_au6_num_0 = np.sum(truth_labels_au6 == 0)\n",
    "truth_labels_au6_num_1 = np.sum(truth_labels_au6 == 1)\n",
    "\n",
    "truth_labels_au12_num_0 = np.sum(truth_labels_au12 == 0)\n",
    "truth_labels_au12_num_1 = np.sum(truth_labels_au12 == 1)\n",
    "\n",
    "species = ('AU 1', 'AU 2', 'AU 4', 'AU 6', 'AU 12')\n",
    "sex_counts = {\n",
    "    'number 0': np.array([truth_labels_au1_num_0,\n",
    "                          truth_labels_au2_num_0,\n",
    "                          truth_labels_au4_num_0,\n",
    "                          truth_labels_au6_num_0,\n",
    "                          truth_labels_au12_num_0]),\n",
    "    'number 1': np.array([truth_labels_au1_num_1,\n",
    "                          truth_labels_au2_num_1,\n",
    "                          truth_labels_au4_num_1,\n",
    "                          truth_labels_au6_num_1,\n",
    "                          truth_labels_au12_num_1]),\n",
    "}\n",
    "width = 0.6  # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(5)\n",
    "\n",
    "for sex, sex_count in sex_counts.items():\n",
    "    p = ax.bar(species, sex_count, width, label=sex, bottom=bottom)\n",
    "    bottom += sex_count\n",
    "\n",
    "    ax.bar_label(p, label_type='center')\n",
    "\n",
    "ax.set_title('Number of penguins by sex')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 伪标签的分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_labels_au1 = pseudo_labels[:, 0]\n",
    "truth_labels_au2 = pseudo_labels[:, 1]\n",
    "truth_labels_au4 = pseudo_labels[:, 2]\n",
    "truth_labels_au6 = pseudo_labels[:, 3]\n",
    "truth_labels_au12 = pseudo_labels[:, 4]\n",
    "\n",
    "truth_labels_au1_num_0 = np.sum(truth_labels_au1 == 0)\n",
    "truth_labels_au1_num_1 = np.sum(truth_labels_au1 == 1)\n",
    "\n",
    "truth_labels_au2_num_0 = np.sum(truth_labels_au2 == 0)\n",
    "truth_labels_au2_num_1 = np.sum(truth_labels_au2 == 1)\n",
    "\n",
    "truth_labels_au4_num_0 = np.sum(truth_labels_au4 == 0)\n",
    "truth_labels_au4_num_1 = np.sum(truth_labels_au4 == 1)\n",
    "\n",
    "truth_labels_au6_num_0 = np.sum(truth_labels_au6 == 0)\n",
    "truth_labels_au6_num_1 = np.sum(truth_labels_au6 == 1)\n",
    "\n",
    "truth_labels_au12_num_0 = np.sum(truth_labels_au12 == 0)\n",
    "truth_labels_au12_num_1 = np.sum(truth_labels_au12 == 1)\n",
    "\n",
    "species = ('AU 1', 'AU 2', 'AU 4', 'AU 6', 'AU 12')\n",
    "sex_counts = {\n",
    "    'number 0': np.array([truth_labels_au1_num_0,\n",
    "                          truth_labels_au2_num_0,\n",
    "                          truth_labels_au4_num_0,\n",
    "                          truth_labels_au6_num_0,\n",
    "                          truth_labels_au12_num_0]),\n",
    "    'number 1': np.array([truth_labels_au1_num_1,\n",
    "                          truth_labels_au2_num_1,\n",
    "                          truth_labels_au4_num_1,\n",
    "                          truth_labels_au6_num_1,\n",
    "                          truth_labels_au12_num_1]),\n",
    "}\n",
    "width = 0.6  # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(5)\n",
    "\n",
    "for sex, sex_count in sex_counts.items():\n",
    "    p = ax.bar(species, sex_count, width, label=sex, bottom=bottom)\n",
    "    bottom += sex_count\n",
    "\n",
    "    ax.bar_label(p, label_type='center')\n",
    "\n",
    "ax.set_title('Number of penguins by sex')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_labels.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度可分离卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = torch.randn(1, 512, 224, 224).cuda()\n",
    "d_conv = nn.Conv2d(512, 256, kernel_size=3, padding=3//2,groups=256).cuda()\n",
    "n_conv = nn.Conv2d(256, 256, kernel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary(d_conv, input_size=(1, 512, 56, 56)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary(n_conv, input_size=(1, 256, 56, 56)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionAdapterV2(nn.Module):\n",
    "    \"\"\"Applies a 1x1 Conv, BatchNorm, ReLU, and AvgPool.\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, pool_kernel, pool_stride):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(in_dim)\n",
    "        self.dw_conv1 = nn.Conv2d(in_dim, in_dim, kernel_size=3, padding=3 // 2, groups=in_dim)\n",
    "        self.dw_conv2 = nn.Conv2d(in_dim, in_dim, kernel_size=5, padding=5 // 2, groups=in_dim)\n",
    "        self.dw_conv3 = nn.Conv2d(in_dim, in_dim, kernel_size=7, padding=7 // 2, groups=in_dim)\n",
    "        \n",
    "        self.conv1x1 = nn.Conv2d(in_dim, out_dim, kernel_size=1)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # Ensure pool output size is consistent if needed, e.g., target 7x7\n",
    "        # Simple AvgPool is used here as in the original code.\n",
    "        self.pool = nn.AvgPool2d(kernel_size=pool_kernel, stride=pool_stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        identity = x\n",
    "        conv1_x = self.dw_conv1(x)\n",
    "        conv2_x = self.dw_conv2(x)\n",
    "        conv3_x = self.dw_conv3(x)\n",
    "        \n",
    "        x = (conv1_x + conv2_x + conv3_x) / 3.0 + identity\n",
    "        \n",
    "        x = self.conv1x1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 2048, 7, 7)\n",
    "in_dim = 2048\n",
    "out_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = VisionAdapterV2(in_dim, out_dim, pool_kernel=1, pool_stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 池化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.AdaptiveAvgPool2d((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=f'res/disfa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-5, 5, 0.1).view(-1, 1)\n",
    "y = -5 * x + 0.1 * torch.randn(x.size())\n",
    "\n",
    "model = torch.nn.Linear(1, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "def train_model(iter):\n",
    "    for epoch in range(iter):\n",
    "        y1 = model(x)\n",
    "        loss = criterion(y1, y)\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "train_model(10)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "writer = SummaryWriter('runs/method_one_example')\n",
    "\n",
    "num_epochs = 3\n",
    "steps_per_epoch = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(steps_per_epoch):\n",
    "        # 模拟训练损失逐渐下降\n",
    "        loss = np.exp(-(epoch * steps_per_epoch + i) / 5.0)\n",
    "        global_step = epoch * steps_per_epoch + i\n",
    "        writer.add_scalar('Loss/train', loss, global_step)\n",
    "        time.sleep(0.1) # 模拟训练过程\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
